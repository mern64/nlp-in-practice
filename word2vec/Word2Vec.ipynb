{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Getting started with Word2Vec in Gensim and making it work!\n",
    "\n",
    "The idea behind Word2Vec is pretty simple. We are making and assumption that you can tell the meaning of a word by the company it keeps. This is analogous to the saying *show me your friends, and I'll tell who you are*. So if you have two words that have very similar neighbors (i.e. the usage context is about the same), then these words are probably quite similar in meaning or are at least highly related. For example, the words `shocked`,`appalled` and `astonished` are typically used in a similar context. \n",
    "\n",
    "In this tutorial, you will learn how to use the Gensim implementation of Word2Vec and actually get it to work! I have heard a lot of complaints about poor performance etc, but its really a combination of two things, (1) your input data and (2) your parameter settings. Note that the training algorithms in this package were ported from the [original Word2Vec implementation by Google](https://arxiv.org/pdf/1301.3781.pdf) and extended with additional functionality."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Imports and logging\n",
    "\n",
    "First, we start with our imports and get logging established:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# imports needed and set up logging\n",
    "import gzip\n",
    "import gensim \n",
    "import logging\n",
    "import pandas as pd\n",
    "\n",
    "logging.basicConfig(format='%(asctime)s : %(levelname)s : %(message)s', level=logging.INFO)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Dataset \n",
    "Next, is our dataset. The secret to getting Word2Vec really working for you is to have lots and lots of text data. In this case I am going to use data from the [OpinRank](http://kavita-ganesan.com/entity-ranking-data/) dataset. This dataset has full user reviews of cars and hotels. I have specifically concatenated all of the hotel reviews into one big file which is about 97MB compressed and 229MB uncompressed. We will use the compressed file for this tutorial. Each line in this file represents a hotel review. You can download the OpinRank Word2Vec dataset here.\n",
    "\n",
    "To avoid confusion, while gensimâ€™s word2vec tutorial says that you need to pass it a sequence of sentences as its input, you can always pass it a whole review as a sentence (i.e. a much larger size of text), and it should not make much of a difference. \n",
    "\n",
    "Now, let's take a closer look at this data below by printing the first line. You can see that this is a pretty hefty review."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2020 ARTICLES \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                                                   0\n",
      "0  this paper quantifies and aggregates the multi...\n",
      "1  a positive demand shock for coltan a mineral w...\n",
      "2  we present results from the first study to exa...\n",
      "3  we study how heterogeneity in performance eval...\n",
      "4  in a field experiment we examine the impact of...\n",
      "Number of entries loaded: 519\n",
      "Example tokenized sentence: ['this', 'paper', 'quantifies', 'and', 'aggregates', 'the', 'multiple', 'lifetime', 'benefits', 'of', 'an', 'influential', 'highquality', 'earlychildhood', 'program', 'with', 'outcomes', 'measured', 'through', 'midlife', 'guided', 'by', 'economic', 'theory', 'we', 'supplement', 'experimental', 'data', 'with', 'nonexperimental', 'data', 'to', 'forecast', 'the', 'lifecycle', 'benefits', 'and', 'costs', 'of', 'the', 'program', 'our', 'point', 'estimate', 'of', 'the', 'internal', 'rate', 'of', 'return', 'is', 'with', 'an', 'associated', 'benefitcost', 'ratio', 'of', 'we', 'account', 'for', 'model', 'estimation', 'and', 'forecasting', 'error', 'and', 'present', 'estimates', 'from', 'extensive', 'sensitivity', 'analyses', 'this', 'paper', 'is', 'a', 'template', 'for', 'synthesizing', 'experimental', 'and', 'nonexperimental', 'data', 'using', 'economic', 'theory', 'to', 'estimate', 'the', 'longrun', 'lifecycle', 'benefits', 'of', 'social', 'programs', 'by', 'the', 'university', 'of', 'chicago', 'all', 'rights', 'reserved']\n"
     ]
    }
   ],
   "source": [
    "# Load the CSV file with only one column (no header)\n",
    "csv_file_path = \"2020 -cleaned_abstracts.csv\"  # Replace with your actual filename\n",
    "df = pd.read_csv(csv_file_path, header=None)\n",
    "\n",
    "# Display the first few entries\n",
    "print(df.head())\n",
    "\n",
    "# Extract the single column as a list of strings\n",
    "text_data = df[0].astype(str).tolist()\n",
    "\n",
    "# Check number of entries\n",
    "print(f\"Number of entries loaded: {len(text_data)}\")\n",
    "\n",
    "# Prepare the training data\n",
    "# Tokenize each sentence into words (basic whitespace tokenization)\n",
    "sentences = [sentence.split() for sentence in text_data]\n",
    "\n",
    "print(f\"Example tokenized sentence: {sentences[0]}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-05-23 11:51:46,703 : INFO : Total number of documents: 519\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "First tokenized sentence: ['this', 'paper', 'quantifies', 'and', 'aggregates', 'the', 'multiple', 'lifetime', 'benefits', 'of', 'an', 'influential', 'highquality', 'earlychildhood', 'program', 'with', 'outcomes', 'measured', 'through', 'midlife', 'guided', 'by', 'economic', 'theory', 'we', 'supplement', 'experimental', 'data', 'with', 'nonexperimental', 'data', 'to', 'forecast', 'the', 'lifecycle', 'benefits', 'and', 'costs', 'of', 'the', 'program', 'our', 'point', 'estimate', 'of', 'the', 'internal', 'rate', 'of', 'return', 'is', 'with', 'an', 'associated', 'benefitcost', 'ratio', 'of', 'we', 'account', 'for', 'model', 'estimation', 'and', 'forecasting', 'error', 'and', 'present', 'estimates', 'from', 'extensive', 'sensitivity', 'analyses', 'this', 'paper', 'is', 'a', 'template', 'for', 'synthesizing', 'experimental', 'and', 'nonexperimental', 'data', 'using', 'economic', 'theory', 'to', 'estimate', 'the', 'longrun', 'lifecycle', 'benefits', 'of', 'social', 'programs', 'by', 'the', 'university', 'of', 'chicago', 'all', 'rights', 'reserved']\n"
     ]
    }
   ],
   "source": [
    "import gensim\n",
    "import logging\n",
    "\n",
    "# Initialize logging\n",
    "logging.basicConfig(format='%(asctime)s : %(levelname)s : %(message)s', level=logging.INFO)\n",
    "\n",
    "# The `sentences` variable is already prepared from earlier (tokenized text)\n",
    "# If you skipped that part, make sure it's defined like this:\n",
    "# sentences = [sentence.split() for sentence in text_data]\n",
    "\n",
    "# Log how many sentences we're working with\n",
    "logging.info(f\"Total number of documents: {len(sentences)}\")\n",
    "\n",
    "# Preview first sentence\n",
    "print(\"First tokenized sentence:\", sentences[0])\n",
    "# Train the Word2Vec model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-05-23 11:52:02,432 : INFO : collecting all words and their counts\n",
      "2025-05-23 11:52:02,433 : INFO : PROGRESS: at sentence #0, processed 0 words, keeping 0 word types\n",
      "2025-05-23 11:52:02,443 : INFO : collected 7225 word types from a corpus of 69613 raw words and 519 sentences\n",
      "2025-05-23 11:52:02,444 : INFO : Creating a fresh vocabulary\n",
      "2025-05-23 11:52:02,456 : INFO : Word2Vec lifecycle event {'msg': 'effective_min_count=2 retains 4472 unique words (61.90% of original 7225, drops 2753)', 'datetime': '2025-05-23T11:52:02.456406', 'gensim': '4.3.3', 'python': '3.12.2 | packaged by Anaconda, Inc. | (main, Feb 27 2024, 17:28:07) [MSC v.1916 64 bit (AMD64)]', 'platform': 'Windows-11-10.0.26100-SP0', 'event': 'prepare_vocab'}\n",
      "2025-05-23 11:52:02,456 : INFO : Word2Vec lifecycle event {'msg': 'effective_min_count=2 leaves 66860 word corpus (96.05% of original 69613, drops 2753)', 'datetime': '2025-05-23T11:52:02.456406', 'gensim': '4.3.3', 'python': '3.12.2 | packaged by Anaconda, Inc. | (main, Feb 27 2024, 17:28:07) [MSC v.1916 64 bit (AMD64)]', 'platform': 'Windows-11-10.0.26100-SP0', 'event': 'prepare_vocab'}\n",
      "2025-05-23 11:52:02,483 : INFO : deleting the raw counts dictionary of 7225 items\n",
      "2025-05-23 11:52:02,484 : INFO : sample=0.001 downsamples 34 most-common words\n",
      "2025-05-23 11:52:02,484 : INFO : Word2Vec lifecycle event {'msg': 'downsampling leaves estimated 52329.08170421639 word corpus (78.3%% of prior 66860)', 'datetime': '2025-05-23T11:52:02.484344', 'gensim': '4.3.3', 'python': '3.12.2 | packaged by Anaconda, Inc. | (main, Feb 27 2024, 17:28:07) [MSC v.1916 64 bit (AMD64)]', 'platform': 'Windows-11-10.0.26100-SP0', 'event': 'prepare_vocab'}\n",
      "2025-05-23 11:52:02,513 : INFO : estimated required memory for 4472 words and 300 dimensions: 12968800 bytes\n",
      "2025-05-23 11:52:02,513 : INFO : resetting layer weights\n",
      "2025-05-23 11:52:02,518 : INFO : Word2Vec lifecycle event {'update': False, 'trim_rule': 'None', 'datetime': '2025-05-23T11:52:02.518056', 'gensim': '4.3.3', 'python': '3.12.2 | packaged by Anaconda, Inc. | (main, Feb 27 2024, 17:28:07) [MSC v.1916 64 bit (AMD64)]', 'platform': 'Windows-11-10.0.26100-SP0', 'event': 'build_vocab'}\n",
      "2025-05-23 11:52:02,519 : INFO : Word2Vec lifecycle event {'msg': 'training model with 10 workers on 4472 vocabulary and 300 features, using sg=1 hs=0 sample=0.001 negative=5 window=10 shrink_windows=True', 'datetime': '2025-05-23T11:52:02.519104', 'gensim': '4.3.3', 'python': '3.12.2 | packaged by Anaconda, Inc. | (main, Feb 27 2024, 17:28:07) [MSC v.1916 64 bit (AMD64)]', 'platform': 'Windows-11-10.0.26100-SP0', 'event': 'train'}\n",
      "2025-05-23 11:52:02,658 : INFO : EPOCH 0: training on 69613 raw words (52377 effective words) took 0.1s, 387183 effective words/s\n",
      "2025-05-23 11:52:02,790 : INFO : EPOCH 1: training on 69613 raw words (52328 effective words) took 0.1s, 396967 effective words/s\n",
      "2025-05-23 11:52:02,932 : INFO : EPOCH 2: training on 69613 raw words (52365 effective words) took 0.1s, 386525 effective words/s\n",
      "2025-05-23 11:52:03,073 : INFO : EPOCH 3: training on 69613 raw words (52376 effective words) took 0.1s, 384924 effective words/s\n",
      "2025-05-23 11:52:03,212 : INFO : EPOCH 4: training on 69613 raw words (52352 effective words) took 0.1s, 372896 effective words/s\n",
      "2025-05-23 11:52:03,212 : INFO : Word2Vec lifecycle event {'msg': 'training on 348065 raw words (261798 effective words) took 0.7s, 374579 effective words/s', 'datetime': '2025-05-23T11:52:03.212774', 'gensim': '4.3.3', 'python': '3.12.2 | packaged by Anaconda, Inc. | (main, Feb 27 2024, 17:28:07) [MSC v.1916 64 bit (AMD64)]', 'platform': 'Windows-11-10.0.26100-SP0', 'event': 'train'}\n",
      "2025-05-23 11:52:03,212 : INFO : Word2Vec lifecycle event {'params': 'Word2Vec<vocab=4472, vector_size=300, alpha=0.025>', 'datetime': '2025-05-23T11:52:03.212774', 'gensim': '4.3.3', 'python': '3.12.2 | packaged by Anaconda, Inc. | (main, Feb 27 2024, 17:28:07) [MSC v.1916 64 bit (AMD64)]', 'platform': 'Windows-11-10.0.26100-SP0', 'event': 'created'}\n",
      "2025-05-23 11:52:03,219 : WARNING : Effective 'alpha' higher than previous training cycles\n",
      "2025-05-23 11:52:03,219 : INFO : Word2Vec lifecycle event {'msg': 'training model with 10 workers on 4472 vocabulary and 300 features, using sg=1 hs=0 sample=0.001 negative=5 window=10 shrink_windows=True', 'datetime': '2025-05-23T11:52:03.219717', 'gensim': '4.3.3', 'python': '3.12.2 | packaged by Anaconda, Inc. | (main, Feb 27 2024, 17:28:07) [MSC v.1916 64 bit (AMD64)]', 'platform': 'Windows-11-10.0.26100-SP0', 'event': 'train'}\n",
      "2025-05-23 11:52:03,351 : INFO : EPOCH 0: training on 69613 raw words (52391 effective words) took 0.1s, 399025 effective words/s\n",
      "2025-05-23 11:52:03,485 : INFO : EPOCH 1: training on 69613 raw words (52309 effective words) took 0.1s, 402098 effective words/s\n",
      "2025-05-23 11:52:03,622 : INFO : EPOCH 2: training on 69613 raw words (52343 effective words) took 0.1s, 391520 effective words/s\n",
      "2025-05-23 11:52:03,768 : INFO : EPOCH 3: training on 69613 raw words (52313 effective words) took 0.1s, 368805 effective words/s\n",
      "2025-05-23 11:52:03,928 : INFO : EPOCH 4: training on 69613 raw words (52228 effective words) took 0.2s, 339236 effective words/s\n",
      "2025-05-23 11:52:04,073 : INFO : EPOCH 5: training on 69613 raw words (52304 effective words) took 0.1s, 368903 effective words/s\n",
      "2025-05-23 11:52:04,229 : INFO : EPOCH 6: training on 69613 raw words (52221 effective words) took 0.1s, 352437 effective words/s\n",
      "2025-05-23 11:52:04,382 : INFO : EPOCH 7: training on 69613 raw words (52319 effective words) took 0.1s, 352633 effective words/s\n",
      "2025-05-23 11:52:04,527 : INFO : EPOCH 8: training on 69613 raw words (52270 effective words) took 0.1s, 368878 effective words/s\n",
      "2025-05-23 11:52:04,666 : INFO : EPOCH 9: training on 69613 raw words (52267 effective words) took 0.1s, 380986 effective words/s\n",
      "2025-05-23 11:52:04,809 : INFO : EPOCH 10: training on 69613 raw words (52331 effective words) took 0.1s, 377140 effective words/s\n",
      "2025-05-23 11:52:04,942 : INFO : EPOCH 11: training on 69613 raw words (52301 effective words) took 0.1s, 400387 effective words/s\n",
      "2025-05-23 11:52:05,080 : INFO : EPOCH 12: training on 69613 raw words (52332 effective words) took 0.1s, 389338 effective words/s\n",
      "2025-05-23 11:52:05,226 : INFO : EPOCH 13: training on 69613 raw words (52416 effective words) took 0.1s, 375046 effective words/s\n",
      "2025-05-23 11:52:05,365 : INFO : EPOCH 14: training on 69613 raw words (52486 effective words) took 0.1s, 378455 effective words/s\n",
      "2025-05-23 11:52:05,504 : INFO : EPOCH 15: training on 69613 raw words (52369 effective words) took 0.1s, 394530 effective words/s\n",
      "2025-05-23 11:52:05,636 : INFO : EPOCH 16: training on 69613 raw words (52262 effective words) took 0.1s, 400121 effective words/s\n",
      "2025-05-23 11:52:05,789 : INFO : EPOCH 17: training on 69613 raw words (52171 effective words) took 0.1s, 357625 effective words/s\n",
      "2025-05-23 11:52:05,928 : INFO : EPOCH 18: training on 69613 raw words (52292 effective words) took 0.1s, 393133 effective words/s\n",
      "2025-05-23 11:52:06,059 : INFO : EPOCH 19: training on 69613 raw words (52401 effective words) took 0.1s, 410330 effective words/s\n",
      "2025-05-23 11:52:06,205 : INFO : EPOCH 20: training on 69613 raw words (52561 effective words) took 0.1s, 358921 effective words/s\n",
      "2025-05-23 11:52:06,351 : INFO : EPOCH 21: training on 69613 raw words (52271 effective words) took 0.1s, 377796 effective words/s\n",
      "2025-05-23 11:52:06,490 : INFO : EPOCH 22: training on 69613 raw words (52386 effective words) took 0.1s, 388819 effective words/s\n",
      "2025-05-23 11:52:06,629 : INFO : EPOCH 23: training on 69613 raw words (52322 effective words) took 0.1s, 391362 effective words/s\n",
      "2025-05-23 11:52:06,768 : INFO : EPOCH 24: training on 69613 raw words (52417 effective words) took 0.1s, 378152 effective words/s\n",
      "2025-05-23 11:52:06,922 : INFO : EPOCH 25: training on 69613 raw words (52257 effective words) took 0.1s, 354287 effective words/s\n",
      "2025-05-23 11:52:07,083 : INFO : EPOCH 26: training on 69613 raw words (52366 effective words) took 0.2s, 335130 effective words/s\n",
      "2025-05-23 11:52:07,231 : INFO : EPOCH 27: training on 69613 raw words (52231 effective words) took 0.1s, 361074 effective words/s\n",
      "2025-05-23 11:52:07,372 : INFO : EPOCH 28: training on 69613 raw words (52257 effective words) took 0.1s, 385288 effective words/s\n",
      "2025-05-23 11:52:07,518 : INFO : EPOCH 29: training on 69613 raw words (52338 effective words) took 0.1s, 365991 effective words/s\n",
      "2025-05-23 11:52:07,518 : INFO : Word2Vec lifecycle event {'msg': 'training on 2088390 raw words (1569732 effective words) took 4.3s, 365123 effective words/s', 'datetime': '2025-05-23T11:52:07.518322', 'gensim': '4.3.3', 'python': '3.12.2 | packaged by Anaconda, Inc. | (main, Feb 27 2024, 17:28:07) [MSC v.1916 64 bit (AMD64)]', 'platform': 'Windows-11-10.0.26100-SP0', 'event': 'train'}\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(1569732, 2088390)"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model = gensim.models.Word2Vec (sentences, vector_size=300, window=10, min_count=2, workers=10,sg=1)\n",
    "model.train(sentences,total_examples=len(sentences),epochs=30)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('sustain', 0.3800406754016876),\n",
       " ('takeoverperformance', 0.34514951705932617),\n",
       " ('takeover', 0.3398277163505554),\n",
       " ('explaining', 0.3389921486377716),\n",
       " ('brightside', 0.3326742351055145),\n",
       " ('bright', 0.3317430913448334),\n",
       " ('disciplinary', 0.32001155614852905),\n",
       " ('collusion', 0.317867636680603),\n",
       " ('lowskilled', 0.3174307346343994),\n",
       " ('mispricing', 0.3174150586128235)]"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "w1 =\"market\"\n",
    "model.wv.most_similar (positive=w1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2010 ARTICLES \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                                                   0\n",
      "0  this paper studies the importance of heterogen...\n",
      "1  in the evaluation of supply chain process impr...\n",
      "2  in the civil aviation market overbooking has b...\n",
      "3  many theories in finance imply monotonic patte...\n",
      "4  we investigate contributions of independent di...\n",
      "Number of entries loaded: 555\n",
      "Example tokenized sentence: ['this', 'paper', 'studies', 'the', 'importance', 'of', 'heterogeneous', 'beliefs', 'for', 'the', 'dynamics', 'of', 'asset', 'prices', 'we', 'focus', 'on', 'currency', 'markets', 'where', 'the', 'absence', 'of', 'shortselling', 'constraints', 'allows', 'us', 'to', 'perform', 'sharper', 'tests', 'of', 'theoretical', 'predictions', 'using', 'a', 'unique', 'data', 'set', 'with', 'detailed', 'information', 'on', 'foreignexchange', 'forecasts', 'we', 'construct', 'an', 'empirical', 'proxy', 'for', 'differences', 'in', 'beliefs', 'we', 'show', 'that', 'this', 'proxy', 'has', 'a', 'strong', 'effect', 'on', 'the', 'implied', 'volatility', 'of', 'currency', 'options', 'beyond', 'the', 'volatility', 'of', 'macroeconomic', 'fundamentals', 'we', 'document', 'that', 'differences', 'in', 'beliefs', 'impact', 'also', 'on', 'the', 'shape', 'of', 'the', 'implied', 'volatility', 'smile', 'on', 'the', 'volatility', 'riskpremiums', 'and', 'on', 'future', 'currency', 'returns', 'elsevier', 'bv']\n"
     ]
    }
   ],
   "source": [
    "# Load the CSV file with only one column (no header)\n",
    "csv_file_path = \"2010 -cleaned_abstracts.csv\"  # Replace with your actual filename\n",
    "df = pd.read_csv(csv_file_path, header=None)\n",
    "\n",
    "# Display the first few entries\n",
    "print(df.head())\n",
    "\n",
    "# Extract the single column as a list of strings\n",
    "text_data = df[0].astype(str).tolist()\n",
    "\n",
    "# Check number of entries\n",
    "print(f\"Number of entries loaded: {len(text_data)}\")\n",
    "\n",
    "# Prepare the training data\n",
    "# Tokenize each sentence into words (basic whitespace tokenization)\n",
    "sentences = [sentence.split() for sentence in text_data]\n",
    "\n",
    "print(f\"Example tokenized sentence: {sentences[0]}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-05-22 20:26:06,710 : INFO : Total number of documents: 555\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "First tokenized sentence: ['this', 'paper', 'studies', 'the', 'importance', 'of', 'heterogeneous', 'beliefs', 'for', 'the', 'dynamics', 'of', 'asset', 'prices', 'we', 'focus', 'on', 'currency', 'markets', 'where', 'the', 'absence', 'of', 'shortselling', 'constraints', 'allows', 'us', 'to', 'perform', 'sharper', 'tests', 'of', 'theoretical', 'predictions', 'using', 'a', 'unique', 'data', 'set', 'with', 'detailed', 'information', 'on', 'foreignexchange', 'forecasts', 'we', 'construct', 'an', 'empirical', 'proxy', 'for', 'differences', 'in', 'beliefs', 'we', 'show', 'that', 'this', 'proxy', 'has', 'a', 'strong', 'effect', 'on', 'the', 'implied', 'volatility', 'of', 'currency', 'options', 'beyond', 'the', 'volatility', 'of', 'macroeconomic', 'fundamentals', 'we', 'document', 'that', 'differences', 'in', 'beliefs', 'impact', 'also', 'on', 'the', 'shape', 'of', 'the', 'implied', 'volatility', 'smile', 'on', 'the', 'volatility', 'riskpremiums', 'and', 'on', 'future', 'currency', 'returns', 'elsevier', 'bv']\n"
     ]
    }
   ],
   "source": [
    "import gensim\n",
    "import logging\n",
    "\n",
    "# Initialize logging\n",
    "logging.basicConfig(format='%(asctime)s : %(levelname)s : %(message)s', level=logging.INFO)\n",
    "\n",
    "# The `sentences` variable is already prepared from earlier (tokenized text)\n",
    "# If you skipped that part, make sure it's defined like this:\n",
    "# sentences = [sentence.split() for sentence in text_data]\n",
    "\n",
    "# Log how many sentences we're working with\n",
    "logging.info(f\"Total number of documents: {len(sentences)}\")\n",
    "\n",
    "# Preview first sentence\n",
    "print(\"First tokenized sentence:\", sentences[0])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Read files into a list\n",
    "Now that we've had a sneak peak of our dataset, we can read it into a list so that we can pass this on to the Word2Vec model. Notice in the code below, that I am directly reading the \n",
    "compressed file. I'm also doing a mild pre-processing of the reviews using `gensim.utils.simple_preprocess (line)`. This does some basic pre-processing such as tokenization, lowercasing, etc and returns back a list of tokens (words). Documentation of this pre-processing method can be found on the official [Gensim documentation site](https://radimrehurek.com/gensim/utils.html). \n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training the Word2Vec model\n",
    "\n",
    "Training the model is fairly straightforward. You just instantiate Word2Vec and pass the reviews that we read in the previous step (the `documents`). So, we are essentially passing on a list of lists. Where each list within the main list contains a set of tokens from a user review. Word2Vec uses all these tokens to internally create a vocabulary. And by vocabulary, I mean a set of unique words.\n",
    "\n",
    "After building the vocabulary, we just need to call `train(...)` to start training the Word2Vec model. Training on the [OpinRank](http://kavita-ganesan.com/entity-ranking-data/) dataset takes about 10 minutes so please be patient while running your code on this dataset.\n",
    "\n",
    "Behind the scenes we are actually training a simple neural network with a single hidden layer. But, we are actually not going to use the neural network after training. Instead, the goal is to learn the weights of the hidden layer. These weights are essentially the word vectors that weâ€™re trying to learn. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-05-22 20:06:29,991 : INFO : collecting all words and their counts\n",
      "2025-05-22 20:06:29,992 : INFO : PROGRESS: at sentence #0, processed 0 words, keeping 0 word types\n",
      "2025-05-22 20:06:30,003 : INFO : collected 11081 word types from a corpus of 71233 raw words and 519 sentences\n",
      "2025-05-22 20:06:30,003 : INFO : Creating a fresh vocabulary\n",
      "2025-05-22 20:06:30,019 : INFO : Word2Vec lifecycle event {'msg': 'effective_min_count=2 retains 5519 unique words (49.81% of original 11081, drops 5562)', 'datetime': '2025-05-22T20:06:30.019507', 'gensim': '4.3.3', 'python': '3.12.2 | packaged by Anaconda, Inc. | (main, Feb 27 2024, 17:28:07) [MSC v.1916 64 bit (AMD64)]', 'platform': 'Windows-11-10.0.26100-SP0', 'event': 'prepare_vocab'}\n",
      "2025-05-22 20:06:30,020 : INFO : Word2Vec lifecycle event {'msg': 'effective_min_count=2 leaves 65671 word corpus (92.19% of original 71233, drops 5562)', 'datetime': '2025-05-22T20:06:30.020511', 'gensim': '4.3.3', 'python': '3.12.2 | packaged by Anaconda, Inc. | (main, Feb 27 2024, 17:28:07) [MSC v.1916 64 bit (AMD64)]', 'platform': 'Windows-11-10.0.26100-SP0', 'event': 'prepare_vocab'}\n",
      "2025-05-22 20:06:30,043 : INFO : deleting the raw counts dictionary of 11081 items\n",
      "2025-05-22 20:06:30,044 : INFO : sample=0.001 downsamples 31 most-common words\n",
      "2025-05-22 20:06:30,044 : INFO : Word2Vec lifecycle event {'msg': 'downsampling leaves estimated 51817.487751786306 word corpus (78.9%% of prior 65671)', 'datetime': '2025-05-22T20:06:30.044523', 'gensim': '4.3.3', 'python': '3.12.2 | packaged by Anaconda, Inc. | (main, Feb 27 2024, 17:28:07) [MSC v.1916 64 bit (AMD64)]', 'platform': 'Windows-11-10.0.26100-SP0', 'event': 'prepare_vocab'}\n",
      "2025-05-22 20:06:30,080 : INFO : estimated required memory for 5519 words and 300 dimensions: 16005100 bytes\n",
      "2025-05-22 20:06:30,080 : INFO : resetting layer weights\n",
      "2025-05-22 20:06:30,085 : INFO : Word2Vec lifecycle event {'update': False, 'trim_rule': 'None', 'datetime': '2025-05-22T20:06:30.085063', 'gensim': '4.3.3', 'python': '3.12.2 | packaged by Anaconda, Inc. | (main, Feb 27 2024, 17:28:07) [MSC v.1916 64 bit (AMD64)]', 'platform': 'Windows-11-10.0.26100-SP0', 'event': 'build_vocab'}\n",
      "2025-05-22 20:06:30,086 : INFO : Word2Vec lifecycle event {'msg': 'training model with 10 workers on 5519 vocabulary and 300 features, using sg=1 hs=0 sample=0.001 negative=5 window=10 shrink_windows=True', 'datetime': '2025-05-22T20:06:30.086214', 'gensim': '4.3.3', 'python': '3.12.2 | packaged by Anaconda, Inc. | (main, Feb 27 2024, 17:28:07) [MSC v.1916 64 bit (AMD64)]', 'platform': 'Windows-11-10.0.26100-SP0', 'event': 'train'}\n",
      "2025-05-22 20:06:30,237 : INFO : EPOCH 0: training on 71233 raw words (51845 effective words) took 0.2s, 338160 effective words/s\n",
      "2025-05-22 20:06:30,381 : INFO : EPOCH 1: training on 71233 raw words (51849 effective words) took 0.1s, 383816 effective words/s\n",
      "2025-05-22 20:06:30,541 : INFO : EPOCH 2: training on 71233 raw words (51857 effective words) took 0.2s, 335711 effective words/s\n",
      "2025-05-22 20:06:30,682 : INFO : EPOCH 3: training on 71233 raw words (51794 effective words) took 0.1s, 377784 effective words/s\n",
      "2025-05-22 20:06:30,814 : INFO : EPOCH 4: training on 71233 raw words (51801 effective words) took 0.1s, 385898 effective words/s\n",
      "2025-05-22 20:06:30,821 : INFO : Word2Vec lifecycle event {'msg': 'training on 356165 raw words (259146 effective words) took 0.7s, 352875 effective words/s', 'datetime': '2025-05-22T20:06:30.821195', 'gensim': '4.3.3', 'python': '3.12.2 | packaged by Anaconda, Inc. | (main, Feb 27 2024, 17:28:07) [MSC v.1916 64 bit (AMD64)]', 'platform': 'Windows-11-10.0.26100-SP0', 'event': 'train'}\n",
      "2025-05-22 20:06:30,821 : INFO : Word2Vec lifecycle event {'params': 'Word2Vec<vocab=5519, vector_size=300, alpha=0.025>', 'datetime': '2025-05-22T20:06:30.821592', 'gensim': '4.3.3', 'python': '3.12.2 | packaged by Anaconda, Inc. | (main, Feb 27 2024, 17:28:07) [MSC v.1916 64 bit (AMD64)]', 'platform': 'Windows-11-10.0.26100-SP0', 'event': 'created'}\n",
      "2025-05-22 20:06:30,821 : WARNING : Effective 'alpha' higher than previous training cycles\n",
      "2025-05-22 20:06:30,821 : INFO : Word2Vec lifecycle event {'msg': 'training model with 10 workers on 5519 vocabulary and 300 features, using sg=1 hs=0 sample=0.001 negative=5 window=10 shrink_windows=True', 'datetime': '2025-05-22T20:06:30.821592', 'gensim': '4.3.3', 'python': '3.12.2 | packaged by Anaconda, Inc. | (main, Feb 27 2024, 17:28:07) [MSC v.1916 64 bit (AMD64)]', 'platform': 'Windows-11-10.0.26100-SP0', 'event': 'train'}\n",
      "2025-05-22 20:06:30,953 : INFO : EPOCH 0: training on 71233 raw words (51828 effective words) took 0.1s, 393116 effective words/s\n",
      "2025-05-22 20:06:31,097 : INFO : EPOCH 1: training on 71233 raw words (51838 effective words) took 0.1s, 387712 effective words/s\n",
      "2025-05-22 20:06:31,248 : INFO : EPOCH 2: training on 71233 raw words (51872 effective words) took 0.1s, 348041 effective words/s\n",
      "2025-05-22 20:06:31,391 : INFO : EPOCH 3: training on 71233 raw words (51810 effective words) took 0.1s, 376894 effective words/s\n",
      "2025-05-22 20:06:31,539 : INFO : EPOCH 4: training on 71233 raw words (51832 effective words) took 0.1s, 354954 effective words/s\n",
      "2025-05-22 20:06:31,697 : INFO : EPOCH 5: training on 71233 raw words (51795 effective words) took 0.2s, 342100 effective words/s\n",
      "2025-05-22 20:06:31,842 : INFO : EPOCH 6: training on 71233 raw words (51767 effective words) took 0.1s, 365665 effective words/s\n",
      "2025-05-22 20:06:31,981 : INFO : EPOCH 7: training on 71233 raw words (51851 effective words) took 0.1s, 369128 effective words/s\n",
      "2025-05-22 20:06:32,145 : INFO : EPOCH 8: training on 71233 raw words (51833 effective words) took 0.2s, 338425 effective words/s\n",
      "2025-05-22 20:06:32,289 : INFO : EPOCH 9: training on 71233 raw words (51818 effective words) took 0.1s, 364011 effective words/s\n",
      "2025-05-22 20:06:32,440 : INFO : EPOCH 10: training on 71233 raw words (51867 effective words) took 0.1s, 356231 effective words/s\n",
      "2025-05-22 20:06:32,593 : INFO : EPOCH 11: training on 71233 raw words (51811 effective words) took 0.1s, 350040 effective words/s\n",
      "2025-05-22 20:06:32,738 : INFO : EPOCH 12: training on 71233 raw words (51908 effective words) took 0.1s, 364308 effective words/s\n",
      "2025-05-22 20:06:32,888 : INFO : EPOCH 13: training on 71233 raw words (51814 effective words) took 0.1s, 355463 effective words/s\n",
      "2025-05-22 20:06:33,045 : INFO : EPOCH 14: training on 71233 raw words (51729 effective words) took 0.2s, 338935 effective words/s\n",
      "2025-05-22 20:06:33,190 : INFO : EPOCH 15: training on 71233 raw words (51750 effective words) took 0.1s, 366680 effective words/s\n",
      "2025-05-22 20:06:33,332 : INFO : EPOCH 16: training on 71233 raw words (51837 effective words) took 0.1s, 374013 effective words/s\n",
      "2025-05-22 20:06:33,489 : INFO : EPOCH 17: training on 71233 raw words (51854 effective words) took 0.2s, 340914 effective words/s\n",
      "2025-05-22 20:06:33,640 : INFO : EPOCH 18: training on 71233 raw words (51780 effective words) took 0.1s, 351816 effective words/s\n",
      "2025-05-22 20:06:33,794 : INFO : EPOCH 19: training on 71233 raw words (51890 effective words) took 0.2s, 342762 effective words/s\n",
      "2025-05-22 20:06:33,946 : INFO : EPOCH 20: training on 71233 raw words (51778 effective words) took 0.1s, 348000 effective words/s\n",
      "2025-05-22 20:06:34,100 : INFO : EPOCH 21: training on 71233 raw words (51870 effective words) took 0.1s, 349339 effective words/s\n",
      "2025-05-22 20:06:34,238 : INFO : EPOCH 22: training on 71233 raw words (51809 effective words) took 0.1s, 385412 effective words/s\n",
      "2025-05-22 20:06:34,392 : INFO : EPOCH 23: training on 71233 raw words (51899 effective words) took 0.1s, 347618 effective words/s\n",
      "2025-05-22 20:06:34,544 : INFO : EPOCH 24: training on 71233 raw words (51798 effective words) took 0.1s, 348603 effective words/s\n",
      "2025-05-22 20:06:34,696 : INFO : EPOCH 25: training on 71233 raw words (51777 effective words) took 0.2s, 341910 effective words/s\n",
      "2025-05-22 20:06:34,842 : INFO : EPOCH 26: training on 71233 raw words (51916 effective words) took 0.1s, 363226 effective words/s\n",
      "2025-05-22 20:06:34,988 : INFO : EPOCH 27: training on 71233 raw words (51765 effective words) took 0.1s, 369903 effective words/s\n",
      "2025-05-22 20:06:35,140 : INFO : EPOCH 28: training on 71233 raw words (51859 effective words) took 0.1s, 354574 effective words/s\n",
      "2025-05-22 20:06:35,287 : INFO : EPOCH 29: training on 71233 raw words (51872 effective words) took 0.1s, 364464 effective words/s\n",
      "2025-05-22 20:06:35,287 : INFO : Word2Vec lifecycle event {'msg': 'training on 2136990 raw words (1554827 effective words) took 4.5s, 348368 effective words/s', 'datetime': '2025-05-22T20:06:35.287168', 'gensim': '4.3.3', 'python': '3.12.2 | packaged by Anaconda, Inc. | (main, Feb 27 2024, 17:28:07) [MSC v.1916 64 bit (AMD64)]', 'platform': 'Windows-11-10.0.26100-SP0', 'event': 'train'}\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(1554827, 2136990)"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model = gensim.models.Word2Vec (sentences, vector_size=300, window=10, min_count=2, workers=10,sg=1)\n",
    "model.train(sentences,total_examples=len(sentences),epochs=30)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Now, let's look at some output \n",
    "This first example shows a simple case of looking up words similar to the word `dirty`. All we need to do here is to call the `most_similar` function and provide the word `dirty` as the positive example. This returns the top 10 similar words. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('sustain', 0.3906792402267456),\n",
       " ('liquidity.', 0.386586993932724),\n",
       " ('low-skilled', 0.37556368112564087),\n",
       " ('increases.', 0.35533690452575684),\n",
       " ('covariances', 0.35038307309150696),\n",
       " ('search.', 0.3463065028190613),\n",
       " ('gradually,', 0.34103667736053467),\n",
       " ('segment', 0.3397790491580963),\n",
       " ('anomalies.', 0.3373395502567291),\n",
       " ('Covariances', 0.3371569514274597)]"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "w1 =\"market\"\n",
    "model.wv.most_similar (positive=w1)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "That looks pretty good, right? Let's look at a few more. Let's look at similarity for `polite`, `france` and `shocked`. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('courteous', 0.9174547791481018),\n",
       " ('friendly', 0.8309274911880493),\n",
       " ('cordial', 0.7990915179252625),\n",
       " ('professional', 0.7945970892906189),\n",
       " ('attentive', 0.7732747197151184),\n",
       " ('gracious', 0.7469891309738159)]"
      ]
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# look up top 6 words similar to 'polite'\n",
    "w1 = [\"polite\"]\n",
    "model.wv.most_similar (positive=w1,topn=6)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('canada', 0.6603403091430664),\n",
       " ('germany', 0.6510637998580933),\n",
       " ('spain', 0.6431018114089966),\n",
       " ('barcelona', 0.61174076795578),\n",
       " ('mexico', 0.6070996522903442),\n",
       " ('rome', 0.6065913438796997)]"
      ]
     },
     "execution_count": 53,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# look up top 6 words similar to 'france'\n",
    "w1 = [\"france\"]\n",
    "model.wv.most_similar (positive=w1,topn=6)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('horrified', 0.80775386095047),\n",
       " ('amazed', 0.7797470092773438),\n",
       " ('astonished', 0.7748459577560425),\n",
       " ('dismayed', 0.7680633068084717),\n",
       " ('stunned', 0.7603034973144531),\n",
       " ('appalled', 0.7466776371002197)]"
      ]
     },
     "execution_count": 54,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# look up top 6 words similar to 'shocked'\n",
    "w1 = [\"shocked\"]\n",
    "model.wv.most_similar (positive=w1,topn=6)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "That's, nice. You can even specify several positive examples to get things that are related in the provided context and provide negative examples to say what should not be considered as related. In the example below we are asking for all items that *relate to bed* only:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('duvet', 0.7086508274078369),\n",
       " ('blanket', 0.7016597390174866),\n",
       " ('mattress', 0.7002605199813843),\n",
       " ('quilt', 0.6868821978569031),\n",
       " ('matress', 0.6777950525283813),\n",
       " ('pillowcase', 0.6413239240646362),\n",
       " ('sheets', 0.6382123827934265),\n",
       " ('foam', 0.6322235465049744),\n",
       " ('pillows', 0.6320573687553406),\n",
       " ('comforter', 0.5972476601600647)]"
      ]
     },
     "execution_count": 55,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# get everything related to stuff on the bed\n",
    "w1 = [\"bed\",'sheet','pillow']\n",
    "w2 = ['couch']\n",
    "model.wv.most_similar (positive=w1,negative=w2,topn=10)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Similarity between two words in the vocabulary"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You can even use the Word2Vec model to return the similarity between two words that are present in the vocabulary. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.76181122646029453"
      ]
     },
     "execution_count": 57,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# similarity between two different words\n",
    "model.wv.similarity(w1=\"dirty\",w2=\"smelly\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1.0000000000000002"
      ]
     },
     "execution_count": 58,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# similarity between two identical words\n",
    "model.wv.similarity(w1=\"dirty\",w2=\"dirty\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.25355593501920781"
      ]
     },
     "execution_count": 59,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# similarity between two unrelated words\n",
    "model.wv.similarity(w1=\"dirty\",w2=\"clean\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Under the hood, the above three snippets computes the cosine similarity between the two specified words using word vectors of each. From the scores, it makes sense that `dirty` is highly similar to `smelly` but `dirty` is dissimilar to `clean`. If you do a similarity between two identical words, the score will be 1.0 as the range of the cosine similarity score will always be between [0.0-1.0]. You can read more about cosine similarity scoring [here](https://en.wikipedia.org/wiki/Cosine_similarity)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Find the odd one out\n",
    "You can even use Word2Vec to find odd items given a list of items."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'france'"
      ]
     },
     "execution_count": 63,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Which one is the odd one out in this list?\n",
    "model.wv.doesnt_match([\"cat\",\"dog\",\"france\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'shower'"
      ]
     },
     "execution_count": 77,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Which one is the odd one out in this list?\n",
    "model.wv.doesnt_match([\"bed\",\"pillow\",\"duvet\",\"shower\"])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Understanding some of the parameters\n",
    "To train the model earlier, we had to set some parameters. Now, let's try to understand what some of them mean. For reference, this is the command that we used to train the model.\n",
    "\n",
    "```\n",
    "model = gensim.models.Word2Vec (documents, size=150, window=10, min_count=2, workers=10)\n",
    "```\n",
    "\n",
    "### `size`\n",
    "The size of the dense vector to represent each token or word. If you have very limited data, then size should be a much smaller value. If you have lots of data, its good to experiment with various sizes. A value of 100-150 has worked well for me. \n",
    "\n",
    "### `window`\n",
    "The maximum distance between the target word and its neighboring word. If your neighbor's position is greater than the maximum window width to the left and the right, then, some neighbors are not considered as being related to the target word. In theory, a smaller window should give you terms that are more related. If you have lots of data, then the window size should not matter too much, as long as its a decent sized window. \n",
    "\n",
    "### `min_count`\n",
    "Minimium frequency count of words. The model would ignore words that do not statisfy the `min_count`. Extremely infrequent words are usually unimportant, so its best to get rid of those. Unless your dataset is really tiny, this does not really affect the model.\n",
    "\n",
    "### `workers`\n",
    "How many threads to use behind the scenes?\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## When should you use Word2Vec?\n",
    "\n",
    "There are many application scenarios for Word2Vec. Imagine if you need to build a sentiment lexicon. Training a Word2Vec model on large amounts of user reviews helps you achieve that. You have a lexicon for not just sentiment, but for most words in the vocabulary. \n",
    "\n",
    "Beyond, raw unstructured text data, you could also use Word2Vec for more structured data. For example, if you had tags for a million stackoverflow questions and answers, you could find tags that are related to a given tag and recommend the related ones for exploration. You can do this by treating each set of co-occuring tags as a \"sentence\" and train a Word2Vec model on this data. Granted, you still need a large number of examples to make it work. \n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
